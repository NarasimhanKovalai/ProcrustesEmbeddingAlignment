{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8wStWECM3vW",
        "outputId": "7a067f91-b073-481d-ac9b-0340a9119453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAzwMtF0Rgdt",
        "outputId": "e146eef8-dd11-4ee8-d6fa-ed2fb9ecb852"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSCH-mf7NsHY",
        "outputId": "1de48954-32e1-4bf6-f834-48f05303920e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4296185 sha256=3ef983b703bb8a64ac81462f222c5492ded792bcec5dcb2d5cb151c2c90e1ee8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3Yu-RBRNuiW"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klQ1cDmoOt30"
      },
      "outputs": [],
      "source": [
        "word_pairs=[]\n",
        "with open('/content/drive/MyDrive/sarvam_project/trunc_lexicons.txt') as f:\n",
        "  for line in f.readlines():\n",
        "    x,y=line.strip().split()\n",
        "    word_pairs.append([x,y])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRV49vtLQbiB"
      },
      "outputs": [],
      "source": [
        "hi_matrix=np.load('/content/drive/MyDrive/sarvam_project/hi_embeddings.npy')\n",
        "en_matrix=np.load('/content/drive/MyDrive/sarvam_project/en_embeddings.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlDYAFt5QlCl"
      },
      "outputs": [],
      "source": [
        "list_en_words=[]\n",
        "with open('/content/drive/MyDrive/sarvam_project/list_en_words.txt') as f:\n",
        "  for line in f.readlines():\n",
        "    list_en_words.append(line.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGG-7YFXR_Hc"
      },
      "outputs": [],
      "source": [
        "list_hi_words=[]\n",
        "with open('/content/drive/MyDrive/sarvam_project/list_hi_words.txt') as f:\n",
        "  for line in f.readlines():\n",
        "    list_hi_words.append(line.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIGjMVi5Qypt"
      },
      "outputs": [],
      "source": [
        "en_aligned=np.load('/content/drive/MyDrive/sarvam_project/en_aligned.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "oWd6SsTTLu_H",
        "outputId": "105e8762-8b45-4a81-dcea-324d9d510e59"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a582905ee3a4>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Find the nearest neighbor for the aligned English word in the Hindi embedding space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mnearest_neighbor_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_nearest_neighbor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Compare the found nearest neighbor with the expected Hindi word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-a582905ee3a4>\u001b[0m in \u001b[0;36mfind_nearest_neighbor\u001b[0;34m(embedding, target_matrix)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Function to find the nearest neighbor based on cosine similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_nearest_neighbor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mnearest_neighbor_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnearest_neighbor_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnearest_neighbor_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1685\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1687\u001b[0;31m     \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_normalized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdense_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     if (\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36missparse\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1513\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1514\u001b[0m     \"\"\"Is `x` of a sparse array or sparse matrix type?\n\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Function to find the nearest neighbor based on cosine similarity\n",
        "def find_nearest_neighbor(embedding, target_matrix):\n",
        "    similarities = cosine_similarity([embedding], target_matrix)[0]\n",
        "    nearest_neighbor_idx = np.argmax(similarities)\n",
        "    return nearest_neighbor_idx, similarities[nearest_neighbor_idx]\n",
        "\n",
        "# Verify alignment quality by comparing aligned English words with Hindi translations\n",
        "correct_matches = 0\n",
        "total_pairs = hi_matrix.shape[0]\n",
        "\n",
        "for i in range(total_pairs):\n",
        "    index_find=list_en_words.index(word_pairs[i][1])\n",
        "    aligned_embedding = en_aligned[index_find]\n",
        "    expected_hindi_embedding = hi_matrix[i]\n",
        "    #print(aligned_embedding.shape,expected_hindi_embedding.shape)\n",
        "\n",
        "    # Find the nearest neighbor for the aligned English word in the Hindi embedding space\n",
        "    nearest_neighbor_idx, similarity_score = find_nearest_neighbor(aligned_embedding, hi_matrix)\n",
        "\n",
        "    # Compare the found nearest neighbor with the expected Hindi word\n",
        "    if (nearest_neighbor_idx==i):\n",
        "        correct_matches += 1\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = correct_matches / total_pairs\n",
        "print(f\"Alignment Accuracy while training: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeT1bSZyRpT0"
      },
      "outputs": [],
      "source": [
        "all_hi_matrix=np.load('/content/drive/MyDrive/sarvam_project/all_hni_matrix.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhd4AwOcRu_8"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/sarvam_project/word_translation_test.txt') as f:\n",
        "    test_word_pairs = [line.strip().split() for line in f.readlines()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h_JUeX1R9N1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOshOvTIQ7U9"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import faiss  # Facebook's ANN library for efficient similarity search\n",
        "\n",
        "# # Preprocess: Build an FAISS index for the Hindi matrix\n",
        "# d = all_hi_matrix.shape[1]  # Dimensionality of the embeddings\n",
        "# index = faiss.IndexFlatIP(d)  # Inner Product (for cosine similarity) index\n",
        "# faiss.normalize_L2(all_hi_matrix)  # Normalize vectors for cosine similarity\n",
        "# index.add(all_hi_matrix)  # Add the Hindi embeddings to the index\n",
        "\n",
        "# correct_matches_at_5 = 0\n",
        "# valid_pairs = [\n",
        "#     pair for pair in test_word_pairs\n",
        "#     if pair[1] in list_en_words and pair[0] in list_hi_words\n",
        "# ]\n",
        "\n",
        "# for pair in valid_pairs:\n",
        "#     en_word, hi_word = pair[1], pair[0]\n",
        "#     en_idx = list_en_words.index(en_word)\n",
        "#     hi_idx = list_hi_words.index(hi_word)\n",
        "\n",
        "#     # Normalize the aligned embedding for cosine similarity\n",
        "#     aligned_embedding = en_aligned[en_idx].reshape(1, -1)\n",
        "#     faiss.normalize_L2(aligned_embedding)\n",
        "\n",
        "#     # Search for the k nearest neighbors\n",
        "#     _, nearest_neighbors_idx = index.search(aligned_embedding, 5)\n",
        "\n",
        "#     # Check if the expected Hindi index is among the neighbors\n",
        "#     if hi_idx in nearest_neighbors_idx[0]:\n",
        "#         correct_matches_at_5 += 1\n",
        "\n",
        "# # Calculate precision@k\n",
        "# precision_at_5 = correct_matches_at_5 / len(valid_pairs)\n",
        "# print(f\"Precision@k (5): {precision_at_5 * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CQUYzgIRdpw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import faiss  # Facebook's ANN library for efficient similarity search\n",
        "\n",
        "def calculate_precision_at_k(\n",
        "    k, all_hi_matrix, en_aligned, test_word_pairs, list_en_words, list_hi_words, index=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate precision@k for word embeddings using FAISS.\n",
        "\n",
        "    Parameters:\n",
        "        k (int): Number of nearest neighbors to consider.\n",
        "        all_hi_matrix (np.ndarray): Hindi word embeddings.\n",
        "        en_aligned (np.ndarray): Aligned English word embeddings.\n",
        "        test_word_pairs (list): List of (Hindi, English) word pairs.\n",
        "        list_en_words (list): List of English words.\n",
        "        list_hi_words (list): List of Hindi words.\n",
        "        index (faiss.IndexFlatIP, optional): Pre-built FAISS index. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        float: Precision@k as a percentage.\n",
        "    \"\"\"\n",
        "    d = all_hi_matrix.shape[1]  # Dimensionality of the embeddings\n",
        "\n",
        "    # Check if the FAISS index already exists, else build one\n",
        "    if index is None:\n",
        "        print(\"[DEBUG] FAISS index does not exist. Creating a new one.\")\n",
        "        index = faiss.IndexFlatIP(d)  # Inner Product (for cosine similarity) index\n",
        "        faiss.normalize_L2(all_hi_matrix)  # Normalize vectors for cosine similarity\n",
        "        index.add(all_hi_matrix)  # Add the Hindi embeddings to the index\n",
        "        print(\"[DEBUG] FAISS index built and embeddings added.\")\n",
        "\n",
        "    # Filter valid pairs\n",
        "    print(\"[DEBUG] Filtering valid pairs from test_word_pairs.\")\n",
        "    valid_pairs = [\n",
        "        pair for pair in test_word_pairs\n",
        "        if pair[1] in list_en_words and pair[0] in list_hi_words\n",
        "    ]\n",
        "    print(f\"[DEBUG] Total valid pairs: {len(valid_pairs)}\")\n",
        "\n",
        "    # Precompute indices for valid pairs\n",
        "    print(\"[DEBUG] Precomputing indices for valid pairs.\")\n",
        "    en_indices = [list_en_words.index(pair[1]) for pair in valid_pairs]\n",
        "    hi_indices = [list_hi_words.index(pair[0]) for pair in valid_pairs]\n",
        "\n",
        "    # Normalize the English embeddings once\n",
        "    print(\"[DEBUG] Normalizing aligned English embeddings.\")\n",
        "    faiss.normalize_L2(en_aligned)\n",
        "\n",
        "    # Batch query the FAISS index\n",
        "    batch_size = 4  # Set an appropriate batch size\n",
        "    correct_matches_at_k = 0\n",
        "\n",
        "    print(\"[DEBUG] Starting batch processing for FAISS index search.\")\n",
        "    for start in range(0, len(en_indices), batch_size):\n",
        "        end = min(start + batch_size, len(en_indices))\n",
        "        batch_indices = en_indices[start:end]\n",
        "        aligned_batch = en_aligned[batch_indices]  # Get batch embeddings\n",
        "\n",
        "        print(f\"[DEBUG] Processing batch: start={start}, end={end}\")\n",
        "\n",
        "        # Search for the k nearest neighbors\n",
        "        _, nearest_neighbors_idx = index.search(aligned_batch, k)\n",
        "\n",
        "        # Count correct matches\n",
        "        for i, hi_idx in enumerate(hi_indices[start:end]):\n",
        "            if hi_idx in nearest_neighbors_idx[i]:\n",
        "                correct_matches_at_k += 1\n",
        "                print(f\"[DEBUG] Match found for index {hi_idx} in batch {start}-{end}\")\n",
        "\n",
        "    # Calculate precision@k\n",
        "    precision_at_k = correct_matches_at_k / len(valid_pairs) * 100\n",
        "    print(f\"[DEBUG] Precision@k ({k}): {precision_at_k:.2f}%\")\n",
        "    return precision_at_k\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZoLc8tiVBLn"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "precision_at_1 = calculate_precision_at_k(\n",
        "    k=1,\n",
        "    all_hi_matrix=all_hi_matrix,\n",
        "    en_aligned=en_aligned,\n",
        "    test_word_pairs=test_word_pairs,\n",
        "    list_en_words=list_en_words,\n",
        "    list_hi_words=list_hi_words,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojRKkHaKVIbJ"
      },
      "outputs": [],
      "source": [
        "precision_at_5 = calculate_precision_at_k(\n",
        "    k=5,\n",
        "    all_hi_matrix=all_hi_matrix,\n",
        "    en_aligned=en_aligned,\n",
        "    test_word_pairs=test_word_pairs,\n",
        "    list_en_words=list_en_words,\n",
        "    list_hi_words=list_hi_words,\n",
        ")\n",
        "print(f\"Precision@1: {precision_at_1:.2f}%\")\n",
        "print(f\"Precision@5: {precision_at_5:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzAZUpGFV6Zv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_top_10_for_first_examples(test_word_pairs, en_aligned, all_hi_matrix, list_en_words, list_hi_words):\n",
        "    \"\"\"\n",
        "    Calculate cosine similarity for the first 10 test word pairs and store top 10 similar items.\n",
        "\n",
        "    Parameters:\n",
        "        test_word_pairs (list): List of (Hindi, English) word pairs.\n",
        "        en_aligned (np.ndarray): Aligned English embeddings.\n",
        "        all_hi_matrix (np.ndarray): Hindi word embeddings.\n",
        "        list_en_words (list): List of English words.\n",
        "        list_hi_words (list): List of Hindi words.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries containing the top 10 results for each test pair.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Normalize the Hindi matrix once for efficiency\n",
        "    all_hi_matrix_norm = all_hi_matrix / np.linalg.norm(all_hi_matrix, axis=1, keepdims=True)\n",
        "\n",
        "    for i, (hindi_word, english_word) in enumerate(test_word_pairs[:10]):  # First 10 examples\n",
        "        if english_word in list_en_words and hindi_word in list_hi_words:\n",
        "            en_idx = list_en_words.index(english_word)\n",
        "            aligned_embedding = en_aligned[en_idx]\n",
        "            hi_idx=list_hi_words.index(hindi_word)\n",
        "            hi_embedding_norm=all_hi_matrix[hi_idx]\n",
        "            # Normalize the query embedding\n",
        "            aligned_embedding_norm = aligned_embedding / np.linalg.norm(aligned_embedding)\n",
        "\n",
        "            # Compute cosine similarity\n",
        "            similarities = np.dot(hi_embedding_norm, aligned_embedding_norm)\n",
        "\n",
        "            score = similarities\n",
        "\n",
        "\n",
        "\n",
        "            results.append(score\n",
        "            )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Example usage\n",
        "top_10_results = calculate_top_10_for_first_examples(\n",
        "    test_word_pairs, en_aligned, all_hi_matrix, list_en_words, list_hi_words\n",
        ")\n",
        "\n",
        "# Print results for the first 10 examples\n",
        "for result in top_10_results:\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uklS1YikskR"
      },
      "outputs": [],
      "source": [
        "discriminator_params={\n",
        "    'emb_dim':300,\n",
        "    'dis_layers':3,\n",
        "    'dis_hid_dim':512,\n",
        "    'dis_dropout':0.0,\n",
        "    'dis_input_dropout':0.1\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsSvd-C1t0Y4",
        "outputId": "0dab6050-c9dc-4ee2-8dd5-be04f61df415"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Discriminator Loss: 0.8928408026695251, Adversarial Loss: 5.5262675285339355\n",
            "Epoch 2, Discriminator Loss: 0.7034378051757812, Adversarial Loss: 2.702890157699585\n",
            "Epoch 3, Discriminator Loss: 0.5627955794334412, Adversarial Loss: 3.695969343185425\n",
            "Epoch 4, Discriminator Loss: 0.07226783782243729, Adversarial Loss: 3.616457223892212\n",
            "Epoch 5, Discriminator Loss: 0.6880297660827637, Adversarial Loss: 2.566481113433838\n",
            "Epoch 6, Discriminator Loss: 0.45557427406311035, Adversarial Loss: 5.310499668121338\n",
            "Epoch 7, Discriminator Loss: 0.4930347502231598, Adversarial Loss: 8.602936744689941\n",
            "Epoch 8, Discriminator Loss: 0.8419936299324036, Adversarial Loss: 1.2795430421829224\n",
            "Epoch 9, Discriminator Loss: 0.525246798992157, Adversarial Loss: 4.500785827636719\n",
            "Epoch 10, Discriminator Loss: 0.13758067786693573, Adversarial Loss: 6.328951358795166\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 300  # Dimension of word embeddings\n",
        "lr = 0.001  # Learning rate\n",
        "num_epochs = 10  # Number of training epochs\n",
        "batch_size = 32\n",
        "\n",
        "# Sample Data (Replace with actual embeddings)\n",
        "# Source and target embeddings as NumPy arrays\n",
        "source_embeddings = en_matrix\n",
        "target_embeddings = hi_matrix  # 1000 target embeddings\n",
        "\n",
        "# Convert to TensorFlow tensors\n",
        "source_embeddings = tf.convert_to_tensor(source_embeddings)\n",
        "target_embeddings = tf.convert_to_tensor(target_embeddings)\n",
        "\n",
        "# Define the Discriminator\n",
        "class Discriminator(Model):\n",
        "    def __init__(self, params):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.emb_dim = params['emb_dim']\n",
        "        self.dis_layers = params['dis_layers']\n",
        "        self.dis_hid_dim = params['dis_hid_dim']\n",
        "        self.dis_dropout = params['dis_dropout']\n",
        "        self.dis_input_dropout = params['dis_input_dropout']\n",
        "\n",
        "        self.model = tf.keras.Sequential()\n",
        "\n",
        "        # Input dropout layer\n",
        "        self.model.add(layers.Dropout(self.dis_input_dropout))\n",
        "\n",
        "        # Add intermediate layers\n",
        "        for i in range(self.dis_layers + 1):\n",
        "            input_dim = self.emb_dim if i == 0 else self.dis_hid_dim\n",
        "            output_dim = 1 if i == self.dis_layers else self.dis_hid_dim\n",
        "\n",
        "            # Dense layer\n",
        "            self.model.add(layers.Dense(output_dim))\n",
        "\n",
        "            # Add activation and dropout for all but the last layer\n",
        "            if i < self.dis_layers:\n",
        "                self.model.add(layers.LeakyReLU(alpha=0.2))\n",
        "                self.model.add(layers.Dropout(self.dis_dropout))\n",
        "\n",
        "        # Final sigmoid activation\n",
        "        self.model.add(layers.Activation('sigmoid'))\n",
        "\n",
        "    def call(self, x):\n",
        "        # Input validation\n",
        "        if len(x.shape) != 2 or x.shape[1] != self.emb_dim:\n",
        "            raise ValueError(f\"Expected input with shape (batch_size, {self.emb_dim}), but got {x.shape}\")\n",
        "        return tf.reshape(self.model(x), [-1])\n",
        "\n",
        "# # Parameters for the discriminator\n",
        "# discriminator_params = {\n",
        "#     'emb_dim': embedding_dim,\n",
        "#     'dis_layers': 2,  # Number of hidden layers\n",
        "#     'dis_hid_dim': 128,  # Hidden layer dimensions\n",
        "#     'dis_dropout': 0.3,  # Dropout rate\n",
        "#     'dis_input_dropout': 0.1  # Input dropout rate\n",
        "# }\n",
        "\n",
        "# Initialize trainable parameters\n",
        "W = tf.Variable(tf.eye(embedding_dim), trainable=True, dtype=tf.float32)  # Mapping matrix\n",
        "\n",
        "discriminator = Discriminator(discriminator_params)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_W = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "optimizer_D = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# Loss function\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(source_embeddings), batch_size):\n",
        "        # Get mini-batches\n",
        "        source_batch = source_embeddings[i:i + batch_size]\n",
        "        target_batch = target_embeddings[i:i + batch_size]\n",
        "\n",
        "        # Train the Discriminator\n",
        "        with tf.GradientTape() as tape_D:\n",
        "            mapped_source = tf.matmul(source_batch, W)\n",
        "            source_preds = discriminator(mapped_source)\n",
        "            target_preds = discriminator(target_batch)\n",
        "\n",
        "            source_labels = tf.ones_like(source_preds)  # Source = 1\n",
        "            target_labels = tf.zeros_like(target_preds)  # Target = 0\n",
        "\n",
        "            loss_D = loss_fn(source_labels, source_preds) + loss_fn(target_labels, target_preds)\n",
        "\n",
        "        gradients_D = tape_D.gradient(loss_D, discriminator.trainable_variables)\n",
        "        optimizer_D.apply_gradients(zip(gradients_D, discriminator.trainable_variables))\n",
        "\n",
        "        # Train the Mapping (W)\n",
        "        with tf.GradientTape() as tape_W:\n",
        "            mapped_source = tf.matmul(source_batch, W)\n",
        "            source_preds = discriminator(mapped_source)\n",
        "\n",
        "            # Reverse labels for adversarial training\n",
        "            target_labels = tf.zeros_like(source_preds)  # Fool discriminator\n",
        "\n",
        "            loss_W = loss_fn(target_labels, source_preds)+loss_fn(source_labels,target_preds)\n",
        "\n",
        "        gradients_W = tape_W.gradient(loss_W, [W])\n",
        "        optimizer_W.apply_gradients(zip(gradients_W, [W]))\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss_D: {loss_D.numpy():.4f}, Loss_W: {loss_W.numpy():.4f}\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVBQ74YrDflS"
      },
      "outputs": [],
      "source": [
        "np.save('/content/drive/MyDrive/sarvam_project/W.npy',W.numpy())\n",
        "all_en_matrix=np.load('/content/drive/MyDrive/sarvam_project/all_en_matrix.npy')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvqTV9yZaRum"
      },
      "outputs": [],
      "source": [
        "W=np.load('/content/drive/MyDrive/sarvam_project/W.npy')\n",
        "en_matrix=np.load('/content/drive/MyDrive/sarvam_project/en_embeddings.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSPmMqbQaPSn"
      },
      "outputs": [],
      "source": [
        "en_aligned_unsupervised_test=np.dot(W,en_matrix.T)\n",
        "en_aligned_unsupervised_test=en_aligned_unsupervised_test.T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSUgU6_eNj60"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "del en_matrix\n",
        "del hi_matrix\n",
        "del all_en_matrix\n",
        "del W\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvvP5cl7ZwSn"
      },
      "outputs": [],
      "source": [
        "all_hi_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhKDCnAEGidQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}